import numpy as np
import tensorflow as tf

from tensorflow.keras import backend as K
from tensorflow.keras.layers import Input

from src.Utils.utils import jaccard_sim


# Tested
def custom_loss(input_x, logits, conv_output, filter_sizes, vocabulary, if_dict):
    """
    Custom loss function to combine the cross entropy with the Jaccard loss
    """
    def loss(y_true, y_pred):
        # Get the ground truth for each word per input text
        # true_features = K.constant(generate_ground_truth(input_x, y_true, vocabulary, if_dict))
        true_features = K.ones_like(input_x)  # TODO: Debugging

        # Get the heatmap generated by interpretation method for each word per input text
        predicted_features = []
        for idx, window_size in enumerate(filter_sizes):
            # use the interpretation method function to generate the heatmap (ex. grad_cam)
            heatmap = grad_cam(input_x, y_true, logits, conv_output[idx])
            word_level_heatmap = word_level_interpretation(heatmap, window_size)
            word_level_heatmap = unify_dim(word_level_heatmap, window_size)
            predicted_features.append(word_level_heatmap)

        # Average the heatmap for the several convolutional filters
        predicted_features = K.sum(predicted_features, axis=0) / len(filter_sizes)

        j_loss, j_acc = jaccard_sim(true_features, predicted_features)

        final_loss = 1e-5 + K.sparse_categorical_crossentropy(y_true, y_pred) + j_loss

        return final_loss

    # Return a function
    return loss


def grad_cam(input_x, input_y, logits, conv_output):
    """
    Compute the grad-cam for a text input
    """
    # shape output (batch_size, sequence_length, num_filters)
    # shape grads_val (batch_size, sequence_length, num_filters)
    output, grads_val = get_gradients(input_x, input_y, logits, conv_output)

    # get the maximum gradient for each gram of words
    # shape (batch_size, sequence_length)
    weights = K.max(grads_val, axis=2)

    # shape cam (batch_size, sequence_length)
    cams = tf.einsum('ijk,ij->ij', output, weights)

    # Process cam
    new_cams = K.zeros_like(cams)
    for i, cam in enumerate(cams):
        # Distance from the mean
        cam_ = cam - cam.mean()

        # Keep only positive values
        cam_ = K.maximum(cam_, 0)

        # Normalize
        cam_ = cam_ / cam_.max() if cam_.max() != 0 else cam_

        new_cams[i] = cam_

    return new_cams


def saliency_map(input_x, input_y, logits, conv_output):
    """
    Compute the saliency map for a text input
    """
    # shape output (batch_size, sequence_length, num_filters)
    # shape grads_val (batch_size, sequence_length, num_filters)
    _, grads_val = get_gradients(input_x, input_y, logits, conv_output)

    # get the maximum gradient for each gram of words
    # shape (batch_size, sequence_length)
    s_maps = K.max(grads_val, axis=2)

    # Process s_maps
    new_s_maps = K.zeros_like(s_maps)
    for i in range(s_maps.shape[0]):
        # Distance from the mean
        s_map_ = s_maps[i] - K.mean(s_maps[i])

        # Keep only positive values
        s_map_ = K.maximum(s_map_, 0)

        # Normalize
        s_map_ = s_map_ / s_map_.max() if s_map_.max() != 0 else s_map_

        new_s_maps[i] = s_map_

    return new_s_maps


def get_gradients(input_x, input_y, logits, conv_output):
    """
    Compute the gradients from the output layer to the target layer
    """
    # convert input_y from one-hot format into index format
    input_y = K.argmax(input_y, axis=1)

    # get the output of the logits not the softmax
    output_y = tf.gather_nd(logits, K.stack([range(input_x.shape[0]), input_y], axis=1))

    # Calculate the gradients from the target class to the target layer
    grads = K.gradients(output_y, conv_output)[0]

    input_shape = (input_x.shape[1],)
    model_input = Input(shape=input_shape, name='input')

    gradient_function = K.function([model_input, K.learning_phase()], [conv_output, grads])

    # shape output (batch_size, sequence_length, num_filters)
    # shape grads_val (batch_size, sequence_length, num_filters)
    output, grads_val = gradient_function([input_x, 0])

    return output, grads_val


def word_level_interpretation(heatmap, window_size):
    """
    distribute the heatmap values from the filter level to the word level of the text input
    :param heatmap: the heatmap values of the input text                # shape [batch_size, sequence_length]
    :param window_size: the window size of the convolutional filter
    """
    word_level_heatmap = np.zeros_like(heatmap)

    for i in range(heatmap.shape[0]):
        for j in range(heatmap.shape[1]):
            for k in range(window_size):
                if j - k >= 0:
                    # Need a bitter method to distribute the weight to the input instead of max
                    word_level_heatmap[i][j] = max(word_level_heatmap[i][j], heatmap[i][j-k])

    return K.constant(word_level_heatmap)


def unify_dim(tensor, window_size):
    """
    unify the tensor to the specific dimension length
    """
    t = K.zeros(shape=(tensor.shape[0], window_size - 1))
    return K.concatenate([tensor, t], axis=1)
